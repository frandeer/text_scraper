import streamlit as st
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium_stealth import stealth
import time
import os
import random
import logging
from datetime import datetime
from bs4 import BeautifulSoup
import glob
import re
import json
import platform
import subprocess
import shutil
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("article_scraper.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("article_scraper")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ê¸°ì‚¬ ì½˜í…ì¸  ìŠ¤í¬ë˜í¼", page_icon="ğŸ”", layout="wide")
st.title("ê¸°ì‚¬ ì½˜í…ì¸  ìŠ¤í¬ë˜í¼")
st.markdown("ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ì˜ ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.")

def detect_site_type(url):
    """URLì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì´íŠ¸ ìœ í˜•ì„ ê°ì§€í•©ë‹ˆë‹¤."""
    if "yozm.wishket.com" in url:
        return "wishket"
    elif "brunch.co.kr" in url:
        return "brunch"
    elif "medium.com" in url:
        return "medium"
    elif "velog.io" in url:
        return "velog"
    else:
        return "unknown"

def get_compatible_chromedriver():
    """
    Streamlit Cloudì™€ í˜¸í™˜ë˜ëŠ” ChromeDriverë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        Service: Chrome WebDriver ì„œë¹„ìŠ¤ ê°ì²´
    """
    try:
        # í™˜ê²½ ê°ì§€
        is_streamlit_cloud = os.environ.get('IS_STREAMLIT_CLOUD') == 'true'
        
        if is_streamlit_cloud:
            logger.info("Streamlit Cloud í™˜ê²½ ê°ì§€ë¨")
            
            # Streamlit Cloudì—ì„œëŠ” ì´ë¯¸ Chromeì´ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©
            CHROME_PATH = "/usr/bin/google-chrome"
            
            # Chrome ë²„ì „ í™•ì¸
            chrome_version = ""
            try:
                chrome_version_cmd = subprocess.run(
                    [CHROME_PATH, "--version"], 
                    capture_output=True, 
                    text=True
                )
                chrome_version = chrome_version_cmd.stdout.strip().split(" ")[-1]
                logger.info(f"ê°ì§€ëœ Chrome ë²„ì „: {chrome_version}")
            except Exception as e:
                logger.error(f"Chrome ë²„ì „ í™•ì¸ ì‹¤íŒ¨: {e}")
                chrome_version = "114.0.5735.90"  # ê¸°ë³¸ ë²„ì „
            
            # ChromeDriver ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì„¤ì •
            chrome_major_version = chrome_version.split('.')[0]
            driver_path = Path("/tmp/chromedriver")
            
            # í™˜ê²½ì— ë§ëŠ” ChromeDriver ì„¤ì¹˜
            if not driver_path.exists():
                logger.info(f"ChromeDriver ì„¤ì¹˜ ì¤‘ (Chrome {chrome_major_version}ìš©)")
                from webdriver_manager.chrome import ChromeDriverManager
                from webdriver_manager.core.utils import ChromeType
                driver_path = ChromeDriverManager(version=chrome_major_version, chrome_type=ChromeType.GOOGLE).install()
            
            logger.info(f"ChromeDriver ê²½ë¡œ: {driver_path}")
            return Service(executable_path=driver_path)
        else:
            # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” webdriver-manager ì‚¬ìš©
            logger.info("ë¡œì»¬ í™˜ê²½ ê°ì§€ë¨, webdriver-manager ì‚¬ìš©")
            return Service(ChromeDriverManager().install())
    
    except Exception as e:
        logger.error(f"ChromeDriver ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ChromeDriverManager ì‚¬ìš©
        return Service(ChromeDriverManager().install())

def setup_chrome_options():
    """Chrome ë¸Œë¼ìš°ì € ì˜µì…˜ì„ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    
    # ë´‡ ê°ì§€ ìš°íšŒë¥¼ ìœ„í•œ ì„¤ì •
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36", 
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ]
    chrome_options.add_argument(f"--user-agent={random.choice(user_agents)}")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option("useAutomationExtension", False)
    
    return chrome_options

def save_page_source(driver, url, output_dir="page_sources"):
    """í˜„ì¬ í˜ì´ì§€ì˜ HTML ì†ŒìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ì‚¬ì´íŠ¸ ìœ í˜• ê°ì§€
    site_type = detect_site_type(url)
    
    # URLì—ì„œ ê¸°ì‚¬ ID ì¶”ì¶œ
    try:
        # URLì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ IDë¡œ ì‚¬ìš©
        article_id = url.strip('/').split('/')[-1]
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
        if '?' in article_id:
            article_id = article_id.split('?')[0]
    except:
        article_id = "unknown"
    
    # íŒŒì¼ëª… ìƒì„±
    filename = f"{output_dir}/{site_type}_article_{article_id}_{timestamp}.html"
    
    # HTML ì†ŒìŠ¤ ì €ì¥
    with open(filename, "w", encoding="utf-8") as f:
        f.write(driver.page_source)
    
    logger.info(f"HTML ì†ŒìŠ¤ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return filename

def extract_content_from_html(html_file):
    """ì €ì¥ëœ HTML íŒŒì¼ì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    logger.info(f"HTML íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ: {html_file}")
    
    try:
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # íŒŒì¼ëª…ì—ì„œ ì‚¬ì´íŠ¸ ìœ í˜• ì¶”ì¶œ
        file_basename = os.path.basename(html_file)
        site_type = "unknown"
        site_types = ["wishket", "brunch", "medium", "velog"]
        for st in site_types:
            if st in file_basename:
                site_type = st
                break
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ (ì‚¬ì´íŠ¸ë³„ ì„ íƒì)
        title_elem = None
        if site_type == "brunch":
            title_elem = soup.select_one('h1.cover_title') or soup.select_one('h1.article_title')
        elif site_type == "medium":
            title_elem = soup.select_one('h1[data-testid="article-title"]') or soup.select_one('h1.pw-post-title')
        elif site_type == "velog":
            title_elem = soup.select_one('h1.head-title')
        
        # ì¼ë°˜ì ì¸ ì œëª© ì„ íƒì (ë‹¤ë¥¸ ì‚¬ì´íŠ¸ìš©)
        if not title_elem:
            for selector in ['h1.article-title', 'h1.post-title', 'h1.entry-title', 'h1.title']:
                title_elem = soup.select_one(selector)
                if title_elem:
                    break
        
        # ê·¸ë˜ë„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ h1 íƒœê·¸ ì‚¬ìš©
        if not title_elem:
            title_elem = soup.select_one('h1')
        
        title = title_elem.text.strip() if title_elem else "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        # ì—¬ëŸ¬ ì»¨í…Œì´ë„ˆ í›„ë³´ íƒìƒ‰ (ì‚¬ì´íŠ¸ë³„ íŠ¹í™”)
        containers = []
        
        # ë¸ŒëŸ°ì¹˜ íŠ¹í™” ì»¨í…Œì´ë„ˆ
        if site_type == "brunch":
            containers.extend([
                soup.select_one('div.wrap_body_frame'),  # ë¸ŒëŸ°ì¹˜ ë©”ì¸ ì»¨í…ì¸ 
                soup.select_one('div.article_body'),     # ë¸ŒëŸ°ì¹˜ ë³¸ë¬¸
                soup.select_one('div.wrap_item')         # ë¸ŒëŸ°ì¹˜ ì•„ì´í…œ ë˜í¼
            ])
        
        # ë¯¸ë””ì—„ íŠ¹í™” ì»¨í…Œì´ë„ˆ
        elif site_type == "medium":
            containers.extend([
                soup.select_one('article'),              # ë¯¸ë””ì—„ ì•„í‹°í´
                soup.select_one('div[data-testid="postContent"]')  # ë¯¸ë””ì—„ í¬ìŠ¤íŠ¸ ì½˜í…ì¸ 
            ])
            
        # ë²¨ë¡œê·¸ íŠ¹í™” ì»¨í…Œì´ë„ˆ
        elif site_type == "velog":
            containers.extend([
                soup.select_one('div.atom-one'),         # ë²¨ë¡œê·¸ ë³¸ë¬¸
                soup.select_one('div.sc-gZMcBi')         # ë²¨ë¡œê·¸ ì»¨í…ì¸ 
            ])
        
        # Wishket íŠ¹í™” ì»¨í…Œì´ë„ˆ
        elif site_type == "wishket":
            containers.extend([
                soup.select_one('div.article-body-container'),  # ìœ„ì‹œì¼“ ê¸°ë³¸ ì„ íƒì
                soup.select_one('div.content-body')             # ìœ„ì‹œì¼“ ëŒ€ì²´ ì„ íƒì
            ])
        
        # ì¼ë°˜ ì»¨í…Œì´ë„ˆ (ëŒ€ë¶€ë¶„ì˜ ì‚¬ì´íŠ¸ì— ì ìš© ê°€ëŠ¥)
        containers.extend([
            soup.select_one('article'),                  # ì¼ë°˜ ì•„í‹°í´ íƒœê·¸
            soup.select_one('main'),                     # ë©”ì¸ íƒœê·¸
            soup.select_one('div.article-content'),      # ì¼ë°˜ ì•„í‹°í´ ì½˜í…ì¸ 
            soup.select_one('div.entry-content'),        # ì¼ë°˜ ì—”íŠ¸ë¦¬ ì½˜í…ì¸ 
            soup.select_one('div.post-content'),         # ì¼ë°˜ í¬ìŠ¤íŠ¸ ì½˜í…ì¸ 
            soup.select_one('div.content')               # ì¼ë°˜ ì½˜í…ì¸ 
        ])
        
        # ìœ íš¨í•œ ì»¨í…Œì´ë„ˆ í•„í„°ë§
        valid_containers = [c for c in containers if c is not None]
        
        if not valid_containers:
            # ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ëŒ€ì²´ ë°©ë²• ì‹œë„
            logger.warning("HTMLì—ì„œ ì£¼ìš” ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²• ì‹œë„...")
            
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
            all_tags = soup.find_all(["p", "div", "article", "section", "main", "span"])
            text_blocks = [(tag, " ".join(tag.text.split())) 
                        for tag in all_tags if tag.text.strip()]
            
            # ê¸¸ì´ìˆœ ì •ë ¬
            text_blocks.sort(key=lambda x: len(x[1]), reverse=True)
            
            if text_blocks:
                # ê°€ì¥ ê¸´ ë¸”ë¡ ì‚¬ìš©
                content = text_blocks[0][1]
                content = clean_content(content, site_type)
                
                logger.info(f"ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ {len(content)}ì ì¶”ì¶œë¨")
                return {
                    'title': title,
                    'content': content,
                    'site_type': site_type
                }
            else:
                return {
                    'title': title, 
                    'content': "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    'site_type': site_type
                }
        
        # ê°€ì¥ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì»¨í…Œì´ë„ˆ ì„ íƒ
        article_container = max(valid_containers, key=lambda c: len(c.text.strip()))
        
        logger.info(f"ì„ íƒëœ ì»¨í…Œì´ë„ˆ: {article_container.name}.{' '.join(article_container.get('class', []))}")
        
        # ë‹¤ì–‘í•œ íƒœê·¸ì—ì„œ ë‚´ìš© ì¶”ì¶œ
        content_elements = []
        
        # p íƒœê·¸ ì¶”ì¶œ (ê¸¸ì´ ì œí•œ ì—†ìŒ)
        p_tags = article_container.select('p')
        for p in p_tags:
            p_text = p.text.strip()
            if p_text:
                content_elements.append(p_text)
        
        # div íƒœê·¸ ì¶”ì¶œ (ê¸¸ì´ ì œí•œ ë‚®ì¶¤: 20ì)
        div_tags = article_container.select('div')
        for div in div_tags:
            div_text = div.text.strip()
            if div_text and len(div_text) > 20:  # ì‹¤ì§ˆì ì¸ ë‚´ìš©ì´ ìˆëŠ” divë§Œ
                # ì´ë¯¸ ì¶”ì¶œëœ ë‚´ìš©ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
                is_duplicate = False
                for existing in content_elements:
                    if div_text in existing or existing in div_text:
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    content_elements.append(div_text)
        
        # span íƒœê·¸ë„ ì¶”ê°€ (ê¸¸ì´ê°€ ê¸´ ê²ƒë§Œ)
        span_tags = article_container.select('span')
        for span in span_tags:
            span_text = span.text.strip()
            if span_text and len(span_text) > 30:  # ì‹¤ì§ˆì ì¸ ë‚´ìš©ì´ ìˆëŠ” spanë§Œ
                # ì´ë¯¸ ì¶”ì¶œëœ ë‚´ìš©ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
                is_duplicate = False
                for existing in content_elements:
                    if span_text in existing or existing in span_text:
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    content_elements.append(span_text)
        
        # li íƒœê·¸ë„ ì¶”ê°€ (ê¸¸ì´ ì œí•œ ì—†ìŒ)
        li_tags = article_container.select('li')
        for li in li_tags:
            li_text = li.text.strip()
            if li_text:
                # ì¤‘ë³µ ê²€ì‚¬
                is_duplicate = False
                for existing in content_elements:
                    if li_text in existing or existing in li_text:
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    content_elements.append(li_text)
        
        # ë¸ŒëŸ°ì¹˜ íŠ¹í™” ì²˜ë¦¬: figure íƒœê·¸ ë‚´ì˜ figcaption ì¶”ê°€
        if site_type == "brunch":
            for fig in article_container.select('figure'):
                caption = fig.select_one('figcaption')
                if caption and caption.text.strip():
                    content_elements.append(f"[ì´ë¯¸ì§€] {caption.text.strip()}")
        
        # ë‚´ìš©ì´ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš° ì»¨í…Œì´ë„ˆ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
        if not content_elements:
            logger.warning("ê°œë³„ ìš”ì†Œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¨í…Œì´ë„ˆ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            content = article_container.text.strip()
        else:
            # ëª¨ë“  ë‚´ìš©ì„ í•©ì³ì„œ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ
            content = "\n\n".join(content_elements)
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        content = clean_content(content, site_type)
        
        logger.info(f"HTML íŒŒì¼ì—ì„œ {len(content)}ì ì¶”ì¶œë¨")
        return {
            'title': title,
            'content': content,
            'site_type': site_type
        }
    
    except Exception as e:
        logger.error(f"HTML íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
        return {
            'error': f"HTML íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }

def get_saved_html_files():
    """ì €ì¥ëœ HTML íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    page_sources = glob.glob("page_sources/*.html")
    error_pages = glob.glob("error_pages/*.html")
    return sorted(page_sources + error_pages, key=os.path.getmtime, reverse=True)

def clean_content(content, site_type="unknown"):
    """ì¶”ì¶œëœ ì½˜í…ì¸ ì—ì„œ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°"""
    # Â©ï¸ ë¬¸ìì—´ ê¸°ì¤€ìœ¼ë¡œ ë‚´ìš© ì˜ë¼ë‚´ê¸°
    for copyright_marker in ["Â©ï¸", "Â©", "â“’", "Copyright", "ì €ì‘ê¶Œ"]:
        if copyright_marker in content:
            content = content.split(copyright_marker)[0]
            break
    
    # ì‚¬ì´íŠ¸ë³„ ì œê±°í•  ë¬¸êµ¬ ëª©ë¡
    common_phrases = [
        "ëª©ë¡ìœ¼ë¡œ",
        "ë³µì‚¬ ì™„ë£Œ!",
        "ê³µìœ í•˜ê¸°",
        "ì¢‹ì•„ìš”",
        "ëŒ“ê¸€",
        "ì‹ ê³ ",
        "êµ¬ë…í•˜ê¸°"
    ]
    
    site_specific_phrases = {
        "wishket": [
            "ìš”ì¦˜ITê°€ PICK í•œ ë‰´ìŠ¤ë ˆí„°ë¥¼ ë§¤ì£¼ ëª©ìš”ì¼ ì— ë§Œë‚˜ë³´ì„¸ìš”.",
            "ê°œì¸ì •ë³´ ìˆ˜ì§‘Â·ì´ìš© ì— ë™ì˜í•´ ì£¼ì„¸ìš”. ë¬´ë£Œë¡œ êµ¬ë…í•˜ê¸°",
            "ìš”ì¦˜IT",
            "ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.",
            "í˜„ì¬ ê¸€",
            "ê´€ë ¨ ê¸€ ë³´ê¸°"
        ],
        "brunch": [
            "ì´ ê¸€ì´ ì¢‹ìœ¼ì…¨ë‹¤ë©´ ì¶”ì²œì„ ëˆŒëŸ¬ì£¼ì„¸ìš”",
            "ì„ íƒí•œ í…ìŠ¤íŠ¸ë¥¼ ë“œë˜ê·¸í•˜ì—¬ í•˜ì´ë¼ì´íŠ¸ í•´ë³´ì„¸ìš”",
            "ê³µìœ í•˜ê¸°",
            "ë¸ŒëŸ°ì¹˜ì—ì„œ ë³´ê¸°",
            "ì‘ê°€ì˜ ê¸€ì„ ê³µìœ í•˜ì„¸ìš”",
            "ì‘ê°€ì˜ ê¸€ì— ê³µê°í•˜ì‹œë©´ â™¡ë¥¼ ëˆ„ë¥´ì„¸ìš”",
            "ì‘ê°€ì •ë³´",
            "You can make anything by writing",
            "C.S.Lewis",
            "ë¸ŒëŸ°ì¹˜ìŠ¤í† ë¦¬ í™ˆ",
            "ë¸ŒëŸ°ì¹˜ìŠ¤í† ë¦¬ ë‚˜ìš°",
            "ë¸ŒëŸ°ì¹˜ìŠ¤í† ë¦¬ ì±…ë°©",
            "ê³„ì •ì„ ìŠì–´ë²„ë¦¬ì…¨ë‚˜ìš”?",
            "ë¡œê·¸ì¸ íšŒì›ê°€ì…"
        ],
        "medium": [
            "Medium is an open platform where",
            "Read more from",
            "More from",
            "Recommended from Medium",
            "Get the Medium app",
            "A button that says 'Download on the App Store'"
        ],
        "velog": [
            "ëŒ“ê¸€ ì‘ì„±í•˜ê¸°",
            "ëŒ“ê¸€ì„ ì‘ì„±í•˜ë ¤ë©´",
            "ë¡œê·¸ì¸",
            "íƒœê·¸",
            "ì‹œë¦¬ì¦ˆì— ì¶”ê°€",
            "ì´ ë¸”ë¡œê·¸ êµ¬ë…í•˜ê¸°"
        ]
    }
    
    # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” ë¬¸êµ¬ ì œê±°
    phrases_to_remove = common_phrases + site_specific_phrases.get(site_type, [])
    
    for phrase in phrases_to_remove:
        content = content.replace(phrase, "")
    
    # ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
    content = ' '.join(content.split())
    
    # ë¬¸ë‹¨ êµ¬ë¶„ì„ ìœ„í•œ ì¤„ë°”ê¿ˆ ì¶”ê°€
    content = content.replace(". ", ".\n\n")
    
    return content.strip()

def scrape_article(url):
    """ì—¬ëŸ¬ ì‚¬ì´íŠ¸ì˜ ê¸°ì‚¬ ë‚´ìš©ì„ ìŠ¤í¬ë©í•˜ëŠ” í•¨ìˆ˜"""
    # ì‚¬ì´íŠ¸ íƒ€ì… ê°ì§€
    site_type = detect_site_type(url)
    logger.info(f"ìŠ¤í¬ë© ì‹œì‘: {url} (ì‚¬ì´íŠ¸ ìœ í˜•: {site_type})")
    
    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = setup_chrome_options()
    
    try:
        with st.spinner('ì›¹ í˜ì´ì§€ ë¡œë”© ì¤‘...'):
            # ChromeDriverManager ëŒ€ì‹  get_compatible_chromedriver í•¨ìˆ˜ ì‚¬ìš©
            service = get_compatible_chromedriver()
            driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # Selenium Stealth ì ìš© (ë´‡ ê°ì§€ íšŒí”¼)
            stealth(
                driver,
                languages=["ko-KR", "ko", "en-US", "en"],
                vendor="Google Inc.",
                platform="Win32",
                webgl_vendor="Intel Inc.",
                renderer="Intel Iris OpenGL Engine",
                fix_hairline=True
            )
            
            # ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ê°ì§€ ë°©ì§€
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            driver.get(url)
            
            # ì‚¬ì´íŠ¸ë³„ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì„¤ì •
            wait_selectors = {
                "wishket": "article",
                "brunch": "div.wrap_body_frame, div.article_body",
                "medium": "article, div[data-testid='postContent']",
                "velog": "div.atom-one, h1.head-title",
                "unknown": "article, main, div.content" 
            }
            
            selector = wait_selectors.get(site_type, wait_selectors["unknown"])
            
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                logger.info("ê¸°ì‚¬ ì»¨í…ì¸  ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ê¸°ì‚¬ ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {e}")
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ë¡œë”© ì½˜í…ì¸  ëŒ€ê¸°)
            time.sleep(3)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥
            page_source_file = save_page_source(driver, url)
            st.info(f"HTML ì†ŒìŠ¤ ì €ì¥ë¨: {page_source_file}")

        # ì‚¬ì´íŠ¸ë³„ ì œëª© ì¶”ì¶œ ì„ íƒì
        title_selectors = {
            "wishket": ["h1.article-title", "h1"],
            "brunch": ["h1.cover_title", "h1.article_title", "h1"],
            "medium": ["h1[data-testid='article-title']", "h1.pw-post-title", "h1"],
            "velog": ["h1.head-title", "h1"]
        }
        
        selectors = title_selectors.get(site_type, ["h1.article-title", "h1.post-title", "h1.entry-title", "h1"])
        
        # ì œëª© ì¶”ì¶œ
        title = "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        for selector in selectors:
            try:
                title_elem = driver.find_element(By.CSS_SELECTOR, selector)
                title = title_elem.text.strip()
                if title:
                    break
            except Exception:
                continue
                
        # ì‚¬ì´íŠ¸ë³„ ë‚´ìš© ì»¨í…Œì´ë„ˆ ì„ íƒì
        content_selectors = {
            "wishket": ["div.article-body-container", "div.content-body"],
            "brunch": ["div.wrap_body_frame", "div.article_body"],
            "medium": ["article", "div[data-testid='postContent']"],
            "velog": ["div.atom-one", "div.sc-gZMcBi"],
            "unknown": ["article", "main", "div.content"]
        }
        
        selectors = content_selectors.get(site_type, content_selectors["unknown"])
        
        # ë‚´ìš© ì¶”ì¶œ
        content = None
        article_container = None
        
        for selector in selectors:
            try:
                article_container = driver.find_element(By.CSS_SELECTOR, selector)
                if article_container:
                    break
            except Exception:
                continue
                
        if article_container:
            try:
                # ë‚´ìš© ìš”ì†Œ ì¶”ì¶œ
                content_elements = []
                
                # p íƒœê·¸ ì¶”ì¶œ
                p_tags = article_container.find_elements(By.TAG_NAME, "p")
                for p in p_tags:
                    if p.text.strip():
                        content_elements.append(p.text)
                        
                # div íƒœê·¸ ì¶”ì¶œ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ìµœì†Œ ê¸¸ì´ í™•ì¸)
                div_tags = article_container.find_elements(By.TAG_NAME, "div")
                for div in div_tags:
                    div_text = div.text.strip()
                    # ë¸ŒëŸ°ì¹˜ëŠ” divì— ì¤‘ìš” ë‚´ìš©ì´ ë§ìœ¼ë¯€ë¡œ ê¸¸ì´ ì œí•œ ì™„í™”
                    min_length = 20 if site_type == "brunch" else 50
                    if div_text and len(div_text) > min_length:
                        # ì´ë¯¸ ì¶”ì¶œëœ ë‚´ìš©ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
                        is_duplicate = False
                        for existing in content_elements:
                            if div_text in existing or existing in div_text:
                                is_duplicate = True
                                break
                        
                        if not is_duplicate:
                            content_elements.append(div_text)
                
                # ë¸ŒëŸ°ì¹˜ íŠ¹í™”: figcaption ì²˜ë¦¬
                if site_type == "brunch":
                    try:
                        figcaptions = article_container.find_elements(By.TAG_NAME, "figcaption")
                        for caption in figcaptions:
                            if caption.text.strip():
                                content_elements.append(f"[ì´ë¯¸ì§€] {caption.text.strip()}")
                    except Exception:
                        pass
                
                # ëª¨ë“  ë‚´ìš©ì„ í•©ì³ì„œ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ
                content = "\n\n".join(content_elements)
                # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                content = clean_content(content, site_type)
                
            except Exception as e:
                logger.error(f"ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                content = None
        
        # ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì €ì¥ëœ HTML íŒŒì¼ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not content:
            st.warning("ì›¹ í˜ì´ì§€ì—ì„œ ì§ì ‘ ë‚´ìš© ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì €ì¥ëœ HTML íŒŒì¼ì—ì„œ ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤...")
            
            html_result = extract_content_from_html(page_source_file)
            if 'error' not in html_result:
                content = html_result['content']
                if title == "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤":
                    title = html_result['title']
                st.success("HTML íŒŒì¼ì—ì„œ ë‚´ìš©ì„ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
            else:
                content = "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                st.error(f"HTML íŒŒì¼ ì¶”ì¶œë„ ì‹¤íŒ¨: {html_result['error']}")
        
        # ë“œë¼ì´ë²„ ì¢…ë£Œ
        driver.quit()
        
        return {
            'title': title,
            'content': content,
            'page_source_file': page_source_file,
            'site_type': site_type
        }

    except Exception as e:
        error_msg = f"ìŠ¤í¬ë© ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg, exc_info=True)
        st.error(error_msg)
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥ ì‹œë„
        if 'driver' in locals():
            try:
                page_source_file = save_page_source(driver, url, "error_pages")
                st.info(f"ì˜¤ë¥˜ ìƒíƒœì˜ HTML ì†ŒìŠ¤ê°€ {page_source_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                driver.quit()
            except Exception as e2:
                logger.error(f"ì˜¤ë¥˜ ì²˜ë¦¬ ì¤‘ ì¶”ê°€ ì˜ˆì™¸ ë°œìƒ: {e2}")
        
        return {'error': str(e)}

def create_copy_button(text, button_text="ë³µì‚¬í•˜ê¸°"):
    """í´ë¦½ë³´ë“œì— ë³µì‚¬í•˜ëŠ” ë²„íŠ¼ ìƒì„±"""
    import json
    from streamlit.components.v1 import html
    
    # í…ìŠ¤íŠ¸ë¥¼ ì´ìŠ¤ì¼€ì´í”„í•˜ì—¬ JavaScriptì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨
    escaped_text = json.dumps(text)
    
    # ë³µì‚¬ ë²„íŠ¼ HTML/JavaScript ì½”ë“œ
    copy_button_html = f"""
    <script>
    function copyToClipboard() {{
        const text = {escaped_text};
        navigator.clipboard.writeText(text)
            .then(() => {{
                const btn = document.getElementById('copyButton');
                btn.innerHTML = 'ë³µì‚¬ ì™„ë£Œ!';
                setTimeout(() => {{
                    btn.innerHTML = '{button_text}';
                }}, 2000);
            }})
            .catch(err => {{
                console.error('í´ë¦½ë³´ë“œ ë³µì‚¬ ì‹¤íŒ¨:', err);
                alert('í´ë¦½ë³´ë“œ ë³µì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
            }});
    }}
    </script>
    <button id="copyButton" onclick="copyToClipboard()" 
        style="background-color: #4CAF50; color: white; border: none; 
        padding: 8px 16px; text-align: center; text-decoration: none; 
        display: inline-block; font-size: 14px; margin: 4px 2px; cursor: pointer; 
        border-radius: 4px;">
        {button_text}
    </button>
    """
    
    # HTML ì»´í¬ë„ŒíŠ¸ë¡œ ë Œë”ë§
    html(copy_button_html, height=50)

# ë©”ì¸ UI
st.sidebar.title("ì˜µì…˜")
mode = st.sidebar.radio("ì‘ì—… ëª¨ë“œ ì„ íƒ", ["ì›¹ ìŠ¤í¬ë˜í•‘", "ì €ì¥ëœ HTML íŒŒì¼ ì½ê¸°"])

if mode == "ì›¹ ìŠ¤í¬ë˜í•‘":
    # URL ì…ë ¥ í•„ë“œ (ê¸°ë³¸ê°’ ì œê±°)
    url = st.text_input("ìŠ¤í¬ë©í•‘í•  ê¸°ì‚¬ URL ì…ë ¥", "")
    
    # ì‚¬ì´íŠ¸ ì˜ˆì‹œ ì œê³µ
    st.caption("ì§€ì› ì‚¬ì´íŠ¸ ì˜ˆì‹œ: ë¸ŒëŸ°ì¹˜(brunch.co.kr), ë¯¸ë””ì—„(medium.com), ë²¨ë¡œê·¸(velog.io) ë“±")
    
    # ê²°ê³¼ ì €ì¥ ë³€ìˆ˜ ì´ˆê¸°í™”
    if 'results' not in st.session_state:
        st.session_state.results = None

    # ìŠ¤í¬ë© ë²„íŠ¼
    if st.button("ìŠ¤í¬ë© ì‹¤í–‰"):
        if url:
            with st.spinner('ê¸°ì‚¬ ìŠ¤í¬ë© ì¤‘...'):
                st.session_state.results = scrape_article(url)
        else:
            st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:  # ì €ì¥ëœ HTML íŒŒì¼ ì½ê¸° ëª¨ë“œ
    html_files = get_saved_html_files()
    
    if not html_files:
        st.warning("ì €ì¥ëœ HTML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ìŠ¤í¬ë©í•´ì£¼ì„¸ìš”.")
    else:
        selected_file = st.selectbox("ë¶„ì„í•  HTML íŒŒì¼ ì„ íƒ", html_files, format_func=lambda x: f"{os.path.basename(x)} ({datetime.fromtimestamp(os.path.getmtime(x)).strftime('%Y-%m-%d %H:%M:%S')})")
        
        if st.button("HTML íŒŒì¼ ë¶„ì„"):
            with st.spinner('HTML íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ ì¤‘...'):
                st.session_state.results = extract_content_from_html(selected_file)
                st.session_state.results['page_source_file'] = selected_file
                st.success("HTML íŒŒì¼ ë¶„ì„ ì™„ë£Œ!")
            
# ê²°ê³¼ í‘œì‹œ
if st.session_state and 'results' in st.session_state and st.session_state.results:
    if 'error' in st.session_state.results:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {st.session_state.results['error']}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ HTML ì†ŒìŠ¤ê°€ ì €ì¥ë˜ì—ˆë‹¤ê³  ì•Œë¦¼
        if 'page_source_file' in st.session_state.results and st.session_state.results['page_source_file']:
            html_path = st.session_state.results['page_source_file']
            st.info(f"ì˜¤ë¥˜ í˜ì´ì§€ HTMLì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {html_path}")
    else:
        # ì‚¬ì´íŠ¸ ìœ í˜• í‘œì‹œ
        site_type = st.session_state.results.get('site_type', 'unknown')
        st.info(f"ì‚¬ì´íŠ¸ ìœ í˜•: {site_type}")
        
        # ì œëª© í‘œì‹œ
        st.subheader(f"ì œëª©: {st.session_state.results['title']}")
        
        # HTML ì†ŒìŠ¤ íŒŒì¼ ì •ë³´
        if 'page_source_file' in st.session_state.results:
            html_path = st.session_state.results['page_source_file']
            st.info(f"HTML ì†ŒìŠ¤: {html_path}")
        
        # ì½˜í…ì¸  í‘œì‹œ
        st.markdown("### ì¶”ì¶œëœ ë‚´ìš©")
        # í…ìŠ¤íŠ¸ ì˜ì—­ìœ¼ë¡œ í‘œì‹œ (ì›ë³¸ ê·¸ëŒ€ë¡œ í‘œì‹œ)
        st.text_area("ì „ì²´ ë‚´ìš©", st.session_state.results['content'], 
                   height=400, disabled=False, key="content")
        
        st.caption(f"ì¶”ì¶œëœ ë‚´ìš© ê¸¸ì´: {len(st.session_state.results['content'])} ê¸€ì")
        
        # ë³µì‚¬ ê¸°ëŠ¥
        st.markdown("### ë‚´ìš© ë³µì‚¬í•˜ê¸°")
        create_copy_button(st.session_state.results['content'], "ë³¸ë¬¸ ë³µì‚¬í•˜ê¸°")

# HTML ë””ë²„ê¹…ì„ ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€
st.sidebar.markdown("---")
st.sidebar.subheader("HTML ë””ë²„ê¹…")
debug_mode = st.sidebar.checkbox("HTML êµ¬ì¡° ë””ë²„ê¹… ëª¨ë“œ")

if debug_mode and 'results' in st.session_state and st.session_state.results and 'page_source_file' in st.session_state.results:
    st.sidebar.markdown("#### HTML ìš”ì†Œ ê²€ì‚¬")
    html_file = st.session_state.results['page_source_file']
    
    if os.path.exists(html_file):
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì‚¬ì´íŠ¸ ìœ í˜•ë³„ ì„ íƒì ì„¤ì •
        site_type = st.session_state.results.get('site_type', 'unknown')
        
        # ì‚¬ì´íŠ¸ë³„ ì£¼ìš” ì»¨í…Œì´ë„ˆ ì„ íƒì
        container_selectors = {
            "wishket": "div.article-body-container, div.content-body",
            "brunch": "div.wrap_body_frame, div.article_body",
            "medium": "article, div[data-testid='postContent']",
            "velog": "div.atom-one, div.sc-gZMcBi",
            "unknown": "div.article-body-container, div.content-body, article, main, div.article-content, div.content"
        }
        
        selector = container_selectors.get(site_type, container_selectors["unknown"])
        
        # ì£¼ìš” êµ¬ì¡° ë¶„ì„
        st.sidebar.markdown(f"##### ì£¼ìš” HTML êµ¬ì¡° ({site_type})")
        main_containers = soup.select(selector)
        
        if main_containers:
            st.sidebar.success(f"{len(main_containers)}ê°œì˜ ì£¼ìš” ì»¨í…Œì´ë„ˆ ì°¾ìŒ")
            container_selector = st.sidebar.selectbox(
                "ë¶„ì„í•  ì»¨í…Œì´ë„ˆ ì„ íƒ", 
                options=range(len(main_containers)),
                format_func=lambda i: f"{main_containers[i].name}.{' '.join(main_containers[i].get('class', []))} ({len(main_containers[i].text)}ì)"
            )
            
            selected_container = main_containers[container_selector]
            
            # ìš”ì†Œë³„ ë¶„ì„
            st.sidebar.markdown("##### ë‚´ë¶€ ìš”ì†Œ ë¶„ì„")
            
            # P íƒœê·¸ ë¶„ì„
            p_tags = selected_container.select("p")
            st.sidebar.text(f"P íƒœê·¸ ìˆ˜: {len(p_tags)}")
            if p_tags:
                p_content = "\n".join([f"{i+1}. ({len(p.text)}ì) {p.text[:50]}..." for i, p in enumerate(p_tags) if p.text.strip()])
                st.sidebar.text_area("P íƒœê·¸ ë¯¸ë¦¬ë³´ê¸°", p_content, height=100)
            
            # DIV íƒœê·¸ ë¶„ì„ (ë‚´ìš©ì´ ìˆëŠ” ê²ƒë§Œ)
            div_tags = [div for div in selected_container.select("div") if div.text.strip() and len(div.text.strip()) > 20]
            st.sidebar.text(f"ì˜ë¯¸ìˆëŠ” DIV íƒœê·¸ ìˆ˜: {len(div_tags)}")
            if div_tags:
                div_content = "\n".join([f"{i+1}. ({len(div.text)}ì) {div.text[:50]}..." for i, div in enumerate(div_tags)])
                st.sidebar.text_area("DIV íƒœê·¸ ë¯¸ë¦¬ë³´ê¸°", div_content, height=100)
            
            # í…ŒìŠ¤íŠ¸ ì¶”ì¶œ
            if st.sidebar.button("ì„ íƒ ì»¨í…Œì´ë„ˆë¡œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"):
                test_elements = []
                # P íƒœê·¸ ì¶”ì¶œ (ê¸¸ì´ ì œí•œ ì—†ìŒ)
                for p in selected_container.select("p"):
                    if p.text.strip():
                        test_elements.append(p.text.strip())
                
                # DIV íƒœê·¸ ì¶”ì¶œ (ê¸¸ì´ ì œí•œ ë‚®ì¶¤: 20ì)
                for div in selected_container.select("div"):
                    div_text = div.text.strip()
                    if div_text and len(div_text) > 20:
                        # ì¤‘ë³µ ë°©ì§€
                        is_duplicate = False
                        for existing in test_elements:
                            if div_text in existing or existing in div_text:
                                is_duplicate = True
                                break
                        
                        if not is_duplicate:
                            test_elements.append(div_text)
                
                # SPAN íƒœê·¸ë„ ì¶”ê°€ (ê¸¸ì´ê°€ ê¸¸ë©´)
                for span in selected_container.select("span"):
                    span_text = span.text.strip()
                    if span_text and len(span_text) > 30:
                        # ì¤‘ë³µ ë°©ì§€
                        is_duplicate = False
                        for existing in test_elements:
                            if span_text in existing or existing in span_text:
                                is_duplicate = True
                                break
                        
                        if not is_duplicate:
                            test_elements.append(span_text)
                
                test_content = "\n\n".join(test_elements)
                test_content = clean_content(test_content)
                
                st.text_area("í…ŒìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼", test_content, height=300)
                st.caption(f"ì¶”ì¶œëœ ë‚´ìš© ê¸¸ì´: {len(test_content)} ê¸€ì")
        else:
            st.sidebar.error("ì£¼ìš” ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ëŒ€ì²´ ë°©ë²• ì œì•ˆ
            st.sidebar.markdown("##### ëŒ€ì²´ ë°©ë²• ì œì•ˆ")
            
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
            all_tags = soup.find_all(["p", "div", "article", "section", "main", "span"])
            text_blocks = [(tag.name, " ".join(tag.text.split()), len(" ".join(tag.text.split()))) 
                          for tag in all_tags if tag.text.strip()]
            
            # ê¸¸ì´ìˆœ ì •ë ¬
            text_blocks.sort(key=lambda x: x[2], reverse=True)
            
            # ìƒìœ„ 5ê°œ ë³´ì—¬ì£¼ê¸°
            st.sidebar.text("ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡:")
            for i, (tag_name, text, length) in enumerate(text_blocks[:5]):
                st.sidebar.text(f"{i+1}. {tag_name}: {length}ì - {text[:50]}...")
                
            # ê°€ì¥ ê¸´ ë¸”ë¡ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            if st.sidebar.button("ê°€ì¥ ê¸´ ë¸”ë¡ìœ¼ë¡œ í…ŒìŠ¤íŠ¸"):
                if text_blocks:
                    longest_text = text_blocks[0][1]
                    st.text_area("ê°€ì¥ ê¸´ ë¸”ë¡ ë‚´ìš©", longest_text, height=300)
                    st.caption(f"ë¸”ë¡ ê¸¸ì´: {len(longest_text)} ê¸€ì")
    else:
        st.sidebar.error(f"HTML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {html_file}")